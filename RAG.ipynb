{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KartikayBhardwaj-dev/Deep_learning_college/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2927677",
      "metadata": {
        "id": "a2927677"
      },
      "source": [
        "# **Building Retrieval-Augmented Generation (RAG) Systems: A Comprehensive Guide**\n",
        "\n",
        "**Course:** Deep Learning and Generative AI  \n",
        "**Institution:** IIT Madras  \n",
        "**Author:** Prof. Balaji Srinivasan  \n",
        "**Date:** November 2025  \n",
        "\n",
        "---\n",
        "\n",
        "## **Learning Objectives**\n",
        "\n",
        "By the end of this tutorial, students will be able to:\n",
        "\n",
        "1. **Understand** the fundamental architecture and components of RAG systems\n",
        "2. **Implement** document processing pipelines for extracting text from PDFs\n",
        "3. **Build** vector databases using embeddings for semantic search\n",
        "4. **Create** retrieval systems that find relevant context for user queries\n",
        "5. **Integrate** large language models with retrieval systems\n",
        "6. **Develop** complete question-answering systems using LangChain\n",
        "7. **Apply** best practices for chunking, embedding, and prompt engineering\n",
        "\n",
        "---\n",
        "\n",
        "## **Prerequisites**\n",
        "\n",
        "- Strong understanding of Python programming and object-oriented concepts\n",
        "- Familiarity with natural language processing fundamentals\n",
        "- Knowledge of embeddings and vector similarity concepts\n",
        "- Understanding of prompt engineering for large language models\n",
        "- Basic experience with APIs and environment variables\n",
        "- Familiarity with text processing and regular expressions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Overview of RAG Systems**"
      ],
      "metadata": {
        "id": "DyBwOPUGedOp"
      },
      "id": "DyBwOPUGedOp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`### **1.1 Introduction to Retrieval-Augmented Generation**\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to produce accurate, contextually relevant answers. Unlike pure LLMs that rely solely on their training data, RAG systems:\n",
        "\n",
        "- **Retrieve** relevant documents from external knowledge bases\n",
        "- **Augment** prompts with retrieved context\n",
        "- **Generate** responses grounded in specific source material\n",
        "- **Reduce hallucinations** by constraining answers to provided context\n",
        "\n",
        "### **1.2 Key Components of RAG Architecture**\n",
        "\n",
        "| **Component** | **Purpose** | **Implementation** |\n",
        "|---------------|-------------|-------------------|\n",
        "| **Document Loader** | Extract text from various sources | PDF readers, web scrapers, database connectors |\n",
        "| **Text Splitter** | Break documents into manageable chunks | Recursive character splitting with overlap |\n",
        "| **Embedding Model** | Convert text to vector representations | Google text-embedding-004 |\n",
        "| **Vector Store** | Enable similarity search over embeddings | FAISS (Facebook AI Similarity Search) |\n",
        "| **Retriever** | Find relevant context for queries | Top-k similarity search |\n",
        "| **Language Model** | Generate natural language responses | Gemini 2.5 Pro via Google AI API |\n",
        "| **Prompt Template** | Structure queries with context | LangChain prompt templates |\n"
      ],
      "metadata": {
        "id": "cR9Aa9kGehjy"
      },
      "id": "cR9Aa9kGehjy"
    },
    {
      "cell_type": "markdown",
      "id": "9abb72fc",
      "metadata": {
        "id": "9abb72fc"
      },
      "source": [
        "## **2. Environment Setup and Dependencies**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **pypdf**: For extracting text from PDF documents\n",
        "- **langchain**: Framework for building LLM applications\n",
        "- **langchain-google-genai**: Google Gemini integrations for LangChain\n",
        "- **google-genai**: Google Generative AI Python SDK\n",
        "- **langchain-community**: Community-maintained integrations\n",
        "- **python-dotenv**: For managing API keys and environment variables"
      ],
      "metadata": {
        "id": "RhD3W5o0etqn"
      },
      "id": "RhD3W5o0etqn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e29528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a9e29528",
        "outputId": "73314e0b-21f0-40ec-8caf-969c5d54ae64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-3.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.52.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<1.0.0,>=0.9.0 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.47)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (2.28.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-3.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, requests, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-text-splitters, google-ai-generativelanguage, langchain-google-genai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.0 filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-google-genai-3.2.0 langchain-text-splitters-1.0.0 marshmallow-3.26.1 mypy-extensions-1.1.0 pypdf-6.4.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "04b193ab8d4343549942c860e175a1ef"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pypdf langchain langchain-google-genai langchain-community python-dotenv google-genai gdown faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af810381",
      "metadata": {
        "id": "af810381"
      },
      "outputs": [],
      "source": [
        "# Core Python libraries\n",
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "import gdown\n",
        "\n",
        "# PDF processing - we'll use pypdf instead of fitz\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# LangChain components for our RAG system\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.docstore.document import Document\n",
        "#from langchain.schema import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Environment management\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load your API keys\n",
        "load_dotenv()\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "xY8rKtq3ciiU"
      },
      "id": "xY8rKtq3ciiU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ee29b7f4",
      "metadata": {
        "id": "ee29b7f4"
      },
      "source": [
        "## **3. Document Processing Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Loading PDF Documents**\n",
        "\n",
        "The first step in building a RAG system is loading documents from external sources. We use pypdf to:\n",
        "- Read PDF files page by page\n",
        "- Extract raw text content\n",
        "- Handle various PDF encodings and formats"
      ],
      "metadata": {
        "id": "snmdVmvcfAyZ"
      },
      "id": "snmdVmvcfAyZ"
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_URL = 'https://drive.google.com/uc?id=1w8ZHRrG5g0lGACvdlBL11bR1DaPldWRA'\n",
        "PDF_PATH = 'hands_on_ml.pdf'\n",
        "\n",
        "gdown.download(PDF_URL, PDF_PATH, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "mOSNkbrZVFcd",
        "outputId": "b4c8f4ed-dd5d-403c-c34f-23fbe12d3e5f"
      },
      "id": "mOSNkbrZVFcd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w8ZHRrG5g0lGACvdlBL11bR1DaPldWRA\n",
            "To: /content/hands_on_ml.pdf\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58.5M/58.5M [00:00<00:00, 75.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hands_on_ml.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d44a062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d44a062",
        "outputId": "917c0935-127c-4fbc-e110-20250a36f58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF loaded with 851 pages\n",
            "Extracted 1704608 characters total\n"
          ]
        }
      ],
      "source": [
        "# Read the PDF and extract all text\n",
        "pdf_reader = PdfReader(PDF_PATH)\n",
        "print(f\"PDF loaded with {len(pdf_reader.pages)} pages\")\n",
        "\n",
        "# Extract text from all pages\n",
        "raw_text = \"\"\n",
        "for page_num, page in enumerate(pdf_reader.pages):\n",
        "    page_text = page.extract_text()\n",
        "    raw_text += page_text\n",
        "\n",
        "print(f\"Extracted {len(raw_text)} characters total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d7674a",
      "metadata": {
        "id": "53d7674a"
      },
      "source": [
        "### **3.2 Text Cleaning and Preprocessing**\n",
        "\n",
        "Raw text extracted from PDFs often contains:\n",
        "- Excessive whitespace from formatting\n",
        "- Control characters from encoding issues\n",
        "- Irregular line breaks and spacing\n",
        "\n",
        "Our cleaning function:\n",
        "- Normalizes whitespace to single spaces\n",
        "- Removes non-printable control characters\n",
        "- Preserves semantic content while improving readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46954cae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46954cae",
        "outputId": "10bb577b-4647-4bf3-fdcf-d9ae7d60c778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: 1659247 characters\n",
            "Preview: Aur√©lien G√©ron Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems TM 2nd Edition Updated for TensorFlow 2 Aur√©lien G√©ron Hands...\n"
          ]
        }
      ],
      "source": [
        "# Clean the extracted text\n",
        "def clean_extracted_text(text: str) -> str:\n",
        "    # Replace multiple whitespace with single spaces\n",
        "    cleaned = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove control characters\n",
        "    cleaned = re.sub(r'[\\x00-\\x1F\\x7F]', '', cleaned)\n",
        "    # Strip leading/trailing whitespace\n",
        "    return cleaned.strip()\n",
        "\n",
        "document_text = clean_extracted_text(raw_text)\n",
        "print(f\"Cleaned text: {len(document_text)} characters\")\n",
        "print(f\"Preview: {document_text[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ab4769",
      "metadata": {
        "id": "a1ab4769"
      },
      "source": [
        "### **3.3 Text Chunking Strategy**\n",
        "\n",
        "Breaking documents into chunks is crucial for RAG systems because:\n",
        "- **LLM Context Limits**: Models have maximum token limits\n",
        "- **Retrieval Precision**: Smaller chunks provide more targeted context\n",
        "- **Semantic Coherence**: Proper chunking preserves meaning\n",
        "\n",
        "**Key Parameters**:\n",
        "- **chunk_size**: Maximum characters per chunk (1000)\n",
        "- **chunk_overlap**: Overlapping characters between chunks (200)\n",
        "- **Separators**: Hierarchical splitting points (paragraphs ‚Üí sentences ‚Üí words)\n",
        "\n",
        "The overlap ensures context isn't lost at chunk boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32040d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d32040d9",
        "outputId": "4d4bf541-efb3-4761-b1aa-ee385a251744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating text chunks...\n",
            "Created 2151 chunks\n"
          ]
        }
      ],
      "source": [
        "chunk_size = 1000\n",
        "chunk_overlap=200\n",
        "\n",
        "# Set up our text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        ")\n",
        "\n",
        "print(\"Creating text chunks...\")\n",
        "text_chunks = text_splitter.split_text(document_text)\n",
        "print(f\"Created {len(text_chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f430ec52",
      "metadata": {
        "id": "f430ec52"
      },
      "source": [
        "### **3.4 Analyzing Chunk Statistics**\n",
        "\n",
        "Understanding your chunks helps optimize retrieval performance:\n",
        "- **Total chunks**: Indicates how many retrievable units exist\n",
        "- **Average size**: Helps verify chunking strategy effectiveness\n",
        "- **Size distribution**: Identifies potential issues with splitting\n",
        "\n",
        "Previewing chunks confirms the content is properly segmented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3598f9c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3598f9c4",
        "outputId": "0bbd436a-e815-44e5-92f7-d538694577de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Statistics:\n",
            "   Total chunks: 2151\n",
            "   Average size: 869 characters\n",
            "   Chunk size range: 1000 characters max\n",
            "   Overlap: 200 characters\n",
            "\n",
            "First chunk preview:\n",
            "Aur√©lien G√©ron Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems TM 2nd Edition Updated for TensorFlow 2 Aur√©lien G√©ron Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow Concepts, Tools, and Techniques to Bui...\n"
          ]
        }
      ],
      "source": [
        "# Show info about our chunks\n",
        "total_chars = sum(len(chunk) for chunk in text_chunks)\n",
        "avg_chunk_size = total_chars / len(text_chunks) if text_chunks else 0\n",
        "\n",
        "print(f\"Chunk Statistics:\")\n",
        "print(f\"   Total chunks: {len(text_chunks)}\")\n",
        "print(f\"   Average size: {avg_chunk_size:.0f} characters\")\n",
        "print(f\"   Chunk size range: {chunk_size} characters max\")\n",
        "print(f\"   Overlap: {chunk_overlap} characters\")\n",
        "\n",
        "# Preview the first chunk\n",
        "print(f\"\\nFirst chunk preview:\")\n",
        "print(f\"{text_chunks[0][:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3367197",
      "metadata": {
        "id": "e3367197"
      },
      "source": [
        "## **4. Building the Vector Database**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1 Setting Up the Embedding Model**\n",
        "\n",
        "Embeddings convert text into dense vector representations that capture semantic meaning. We use Google's `text-embedding-004` model:\n",
        "\n",
        "- **Dimensionality**: 768 dimensions\n",
        "- **Performance**: State-of-the-art quality and speed\n",
        "- **Cost-effective**: Free tier available with generous quotas\n",
        "- **Semantic understanding**: Captures deep contextual relationships\n",
        "- **Multilingual**: Supports 100+ languages\n",
        "\n",
        "The embedding model is the foundation of similarity search."
      ],
      "metadata": {
        "id": "0WWs8sZlfFLh"
      },
      "id": "0WWs8sZlfFLh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5522272",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5522272",
        "outputId": "a56a9b55-efc6-4653-b0a4-5e7491c70446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up embeddings model...\n",
            "Embeddings model ready!\n"
          ]
        }
      ],
      "source": [
        "# Set up the embedding model\n",
        "print(\"Setting up embeddings model...\")\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\",  # Google's latest embedding model\n",
        "    task_type=\"retrieval_document\"\n",
        ")\n",
        "print(\"Embeddings model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd5f50e",
      "metadata": {
        "id": "edd5f50e"
      },
      "source": [
        "### **4.2 Creating Document Objects**\n",
        "\n",
        "LangChain's Document objects structure our data for the RAG pipeline:\n",
        "\n",
        "**Components**:\n",
        "- **page_content**: The actual text content of the chunk\n",
        "- **metadata**: Additional information for tracking and filtering\n",
        "  - chunk_id: Unique identifier for each chunk\n",
        "  - chunk_length: Character count for analysis\n",
        "  - source: Origin of the document\n",
        "\n",
        "Metadata enables sophisticated retrieval strategies and source attribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "394dd650",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "394dd650",
        "outputId": "ac0b1321-6d1c-4730-9edb-f99598f6c020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting chunks to documents...\n",
            "Created 2151 document objects\n",
            "\n",
            "Sample document:\n",
            "   Content length: 956\n",
            "   Metadata: {'chunk_id': 0, 'chunk_length': 956, 'source': 'pdf_document'}\n",
            "   Preview: Aur√©lien G√©ron Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow Concepts, Tools, and Techniques to Build Intelligent Systems TM 2nd Edi...\n"
          ]
        }
      ],
      "source": [
        "# Convert chunks to LangChain documents\n",
        "print(\"Converting chunks to documents...\")\n",
        "documents = []\n",
        "\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    doc = Document(\n",
        "        page_content=chunk,\n",
        "        metadata={\n",
        "            \"chunk_id\": i,\n",
        "            \"chunk_length\": len(chunk),\n",
        "            \"source\": \"pdf_document\"\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"Created {len(documents)} document objects\")\n",
        "\n",
        "# Show a sample document\n",
        "sample_doc = documents[0]\n",
        "print(f\"\\nSample document:\")\n",
        "print(f\"   Content length: {len(sample_doc.page_content)}\")\n",
        "print(f\"   Metadata: {sample_doc.metadata}\")\n",
        "print(f\"   Preview: {sample_doc.page_content[:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaning documents before embedding...\")\n",
        "\n",
        "# This loops through all your documents and cleans their content\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.encode('utf-8', 'replace').decode('utf-8')\n",
        "\n",
        "print(\"Cleaning complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMrgUCvteFJ1",
        "outputId": "0364b693-859e-46ec-ee10-4418aba458b5"
      },
      "id": "SMrgUCvteFJ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning documents before embedding...\n",
            "Cleaning complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821ce006",
      "metadata": {
        "id": "821ce006"
      },
      "source": [
        "### **4.3 Building the FAISS Vector Store**\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) provides efficient similarity search:\n",
        "\n",
        "**Process**:\n",
        "1. **Embed documents**: Convert each chunk to a vector using the embedding model\n",
        "2. **Index vectors**: Build an efficient search index structure\n",
        "3. **Enable retrieval**: Allow fast k-nearest neighbor queries\n",
        "\n",
        "**Performance**: FAISS can handle millions of vectors with sub-second query times.\n",
        "\n",
        "**Note**: This step calls the Google Gemini API for each chunk and may take several minutes depending on document size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551fd333",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "551fd333",
        "outputId": "4a2a08de-5eca-47e0-cd6c-b64b9db969d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building searchable vector database...\n",
            "This might take a few minutes...\n",
            "Vector database created successfully!\n",
            "Indexed 2151 document chunks\n"
          ]
        }
      ],
      "source": [
        "# Create the vector database\n",
        "print(\"Building searchable vector database...\")\n",
        "print(\"This might take a few minutes...\")\n",
        "\n",
        "vector_store = FAISS.from_documents(\n",
        "    documents=documents,      # These are now the cleaned documents\n",
        "    embedding=embeddings_model\n",
        ")\n",
        "\n",
        "print(\"Vector database created successfully!\")\n",
        "print(f\"Indexed {len(documents)} document chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa69fe0",
      "metadata": {
        "id": "4aa69fe0"
      },
      "source": [
        "### **4.4 Testing Vector Search**\n",
        "\n",
        "Before building the complete RAG chain, verify the retrieval system:\n",
        "\n",
        "**Similarity Search Process**:\n",
        "1. Embed the query using the same embedding model\n",
        "2. Compute cosine similarity between query and all document vectors\n",
        "3. Return top-k most similar chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58c5d05",
      "metadata": {
        "id": "a58c5d05"
      },
      "outputs": [],
      "source": [
        "# Test the search functionality\n",
        "def test_vector_search(query: str, num_results: int = 3):\n",
        "    print(f\"üîç Searching for: '{query}'\")\n",
        "\n",
        "    # Perform similarity search\n",
        "    search_results = vector_store.similarity_search(\n",
        "        query=query,\n",
        "        k=num_results\n",
        "    )\n",
        "\n",
        "    print(f\"üìã Found {len(search_results)} relevant chunks:\")\n",
        "\n",
        "    for i, doc in enumerate(search_results, 1):\n",
        "        print(f\"\\nüìÑ Result {i}:\")\n",
        "        print(f\"   Chunk ID: {doc.metadata.get('chunk_id', 'unknown')}\")\n",
        "        print(f\"   Preview: {doc.page_content[:200]}...\")\n",
        "\n",
        "    return search_results\n",
        "\n",
        "# Test with a sample question\n",
        "test_query = \"What is machine learning?\"\n",
        "search_results = test_vector_search(test_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4871a2",
      "metadata": {
        "id": "8c4871a2"
      },
      "source": [
        "## **5. Integrating the Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Configuring the LLM**\n",
        "\n",
        "We use Google's Gemini 2.0 Flash as our generation model:\n",
        "\n",
        "**Configuration**:\n",
        "- **model**: \"gemini-2.5-pro\" - Highly capable model with extended context\n",
        "- **temperature**: 0.0 - Deterministic, factual responses (reduces creativity/hallucination)"
      ],
      "metadata": {
        "id": "VxE4zzjgfw5N"
      },
      "id": "VxE4zzjgfw5N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26839d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26839d1f",
        "outputId": "d6b58f1e-2d33-4d17-b7b3-49ee910e07b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up AI language model...\n",
            "Language model ready!\n"
          ]
        }
      ],
      "source": [
        "# Set up the language model\n",
        "print(\"Setting up AI language model...\")\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0.3  # Low temperature for consistent, factual answers\n",
        ")\n",
        "print(\"Language model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb21826",
      "metadata": {
        "id": "fdb21826"
      },
      "source": [
        "### **5.2 Designing the Prompt Template**\n",
        "\n",
        "The prompt is crucial for RAG system performance. Our template:\n",
        "\n",
        "**Instructions to the Model**:\n",
        "1. **Only use provided context** - Prevents hallucination\n",
        "2. **Admit limitations** - Be honest when context is insufficient\n",
        "3. **Cite sources** - Reference relevant parts of context\n",
        "4. **Stay concise** - Avoid unnecessary elaboration\n",
        "5. **Don't guess** - Better to say \"I don't know\"\n",
        "\n",
        "**Structure**:\n",
        "- System role instructions\n",
        "- Context placeholder (filled with retrieved chunks)\n",
        "- User question\n",
        "- Response directive\n",
        "\n",
        "This design ensures answers are grounded in source material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef4f00d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ef4f00d",
        "outputId": "2382d653-f19e-443e-d566-a29e99bcdcb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt template created!\n"
          ]
        }
      ],
      "source": [
        "# Create the prompt template using LCEL\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful AI assistant that answers questions based on the provided context.\n",
        "\n",
        "Rules:\n",
        "1. Only use information from the provided context to answer questions\n",
        "2. If the context doesn't contain enough information, say so honestly\n",
        "3. Be specific and cite relevant parts of the context\n",
        "4. Keep your answers clear and concise\n",
        "5. If you're unsure, admit it rather than guessing\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer based on the context above:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(system_prompt)\n",
        "print(\"Prompt template created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee2e5a5",
      "metadata": {
        "id": "3ee2e5a5"
      },
      "source": [
        "## **6. Building the Complete RAG Chain**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1 LangChain Expression Language (LCEL)**\n",
        "\n",
        "We use LCEL to build a composable RAG pipeline:\n",
        "\n",
        "**Pipeline Components**:\n",
        "1. **Retriever**: Finds top-k relevant chunks (`search_kwargs={\"k\": 4}`)\n",
        "2. **format_docs**: Combines retrieved chunks with double newlines\n",
        "3. **Prompt Template**: Structures context + question\n",
        "4. **LLM**: Generates response based on prompt\n",
        "5. **Output Parser**: Extracts string from LLM response\n",
        "\n",
        "**LCEL Syntax**:\n",
        "- `|` operator chains components\n",
        "- `{}` creates parallel execution\n",
        "- `RunnablePassthrough()` forwards input unchanged\n",
        "\n",
        "This creates an end-to-end system: **Question ‚Üí Retrieval ‚Üí Context ‚Üí LLM ‚Üí Answer**"
      ],
      "metadata": {
        "id": "uTXggFHCgIr8"
      },
      "id": "uTXggFHCgIr8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba795a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ba795a3",
        "outputId": "4a4599af-6929-4a29-e660-7d9c9c393e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete RAG system ready!\n",
            "You can now ask questions about your document!\n"
          ]
        }
      ],
      "source": [
        "# Import LCEL components\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Create retriever from our vector store\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
        ")\n",
        "\n",
        "# Define a function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Build the RAG chain using LCEL pipe syntax\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,  # Retrieve docs and format them\n",
        "        \"input\": RunnablePassthrough()       # Pass the question through\n",
        "    }\n",
        "    | prompt_template                        # Format the prompt with context and question\n",
        "    | llm                                   # Send to language model\n",
        "    | StrOutputParser()                     # Parse the output to a string\n",
        ")\n",
        "\n",
        "print(\"Complete RAG system ready!\")\n",
        "print(\"You can now ask questions about your document!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d137b32",
      "metadata": {
        "id": "6d137b32"
      },
      "source": [
        "## **7. Using the RAG System**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1 Interactive Question-Answering**"
      ],
      "metadata": {
        "id": "qZYcaFfYgRsS"
      },
      "id": "qZYcaFfYgRsS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb7544e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb7544e",
        "outputId": "02ca586c-635e-4e07-c309-98ec1a426a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is Machine Learning?\n",
            "Thinking...\n",
            "\n",
            "Answer:\n",
            "Based on the context provided, Machine Learning is defined in a few ways:\n",
            "\n",
            "*   It is \"the science (and art) of programming computers so they can learn from data.\"\n",
            "*   A more general definition from Arthur Samuel (1959) describes it as the \"field of study that gives computers the ability to learn without being explicitly programmed.\"\n",
            "*   A more engineering-oriented definition from Tom Mitchell (1997) states: \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\"\n",
            "\n",
            "Based on 4 source chunks:\n",
            "\n",
            "Source 1 (Chunk 4):\n",
            "   . End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
            "\n",
            "Source 2 (Chunk 2):\n",
            "   . The views expressed in this work are those of the author, and do not represent the publisher‚Äôs views. While the publisher and the author have used good faith efforts to ensure that the information a...\n",
            "\n",
            "Source 3 (Chunk 56):\n",
            "   . What Is Machine Learning? Machine Learning is the science (and art) of programming computers so they can learn from data. Here is a slightly more general definition: [Machine Learning is the] field ...\n",
            "\n",
            "Source 4 (Chunk 54):\n",
            "   . It‚Äôs not exactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has actually learned so well that you seldom need to flag an email as spam anymore). It was followed by ...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def ask_document_question(question: str):\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"Thinking...\")\n",
        "\n",
        "    # Get the answer from our RAG system\n",
        "    # With LCEL, we pass the question directly as a string\n",
        "    response = rag_chain.invoke(question)\n",
        "\n",
        "    # Display the answer\n",
        "    print(f\"\\nAnswer:\")\n",
        "    print(f\"{response}\")\n",
        "\n",
        "    # To see source documents, we need to get them separately\n",
        "    source_docs = retriever.invoke(question)\n",
        "    print(f\"\\nBased on {len(source_docs)} source chunks:\")\n",
        "\n",
        "    for i, doc in enumerate(source_docs, 1):\n",
        "        chunk_id = doc.metadata.get('chunk_id', 'unknown')\n",
        "        print(f\"\\nSource {i} (Chunk {chunk_id}):\")\n",
        "        print(f\"   {doc.page_content[:200]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    return response\n",
        "\n",
        "# Test with some questions\n",
        "questions = [\n",
        "    \"What is Machine Learning?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = ask_document_question(question)\n",
        "    print()  # Add some space between questions"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DyBwOPUGedOp",
        "9abb72fc",
        "ee29b7f4",
        "e3367197",
        "8c4871a2",
        "3ee2e5a5",
        "6d137b32"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}